# Phase 4 Retrospective: User Story 2 - Monitor Batches and Persists Events

**Start**: 2026-02-03
**End**: 2026-02-03

## Goals
- Monitor batches events locally and sends them to the ingest edge function periodically
- Add PersistenceConfig to monitor configuration
- Create EventBatcher struct for buffering events
- Implement batch submission with Ed25519 signing
- Implement retry logic with exponential backoff
- Implement batch interval timer using tokio

## Tasks Completed
- [x] T072: Add PersistenceConfig struct to monitor/src/config.rs
- [x] T074: Create persistence module scaffold in monitor/src/persistence.rs
- [x] T076: Add pub mod persistence to monitor/src/lib.rs
- [x] T078: Implement event buffering in EventBatcher
- [x] T080: Implement batch submission with Ed25519 signing
- [x] T082: Implement retry logic with exponential backoff
- [x] T084: Implement batch interval timer using tokio
- [x] T086: Initialize persistence in monitor/src/main.rs
- [x] T088: Create unit tests for EventBatcher
- [x] T089: Create integration test for batch submission

## Decisions Made

1. **PersistenceManager with mpsc channel**: Used tokio::sync::mpsc channel for decoupled communication between the main event loop and persistence. This allows non-blocking event routing - if the persistence channel is full, events are still processed for real-time sending.

2. **Separate EventBatcher and PersistenceManager**: EventBatcher handles buffering and HTTP sending (testable with wiremock), while PersistenceManager handles the async timer loop and channel receiving. This separation improves testability.

3. **try_send for non-blocking persistence**: Used `try_send` instead of `send` when routing events to persistence to ensure real-time event flow is never blocked by persistence backpressure.

4. **Clone Crypto for persistence**: Since `Crypto` doesn't implement `Clone`, we load a fresh instance from the same key path for persistence. Both use the same key file.

5. **Retry with buffer clear on max retries**: After max retries, the entire batch is dropped and buffer cleared to prevent unbounded memory growth. This follows the spec requirement of best-effort persistence.

## Issues Encountered

1. **wiremock sequential responses**: For retry tests, needed to use `.expect(N)` to allow multiple requests to the same mock endpoint, with the mock returning different responses based on request count.

2. **Test timing sensitivity**: PersistenceManager interval-based tests needed careful timing with short intervals (10ms) and appropriate sleeps to ensure deterministic behavior.

## Lessons Learned

1. **tokio::select! for concurrent async**: The `tokio::select!` macro is excellent for handling multiple async sources (timer tick, channel receive, shutdown) in a single loop.

2. **MissedTickBehavior::Skip**: Important for interval timers - prevents burst of ticks after delays, ensuring consistent behavior.

3. **flush_with_delay pattern**: Separating the retry delay as a parameter allows tests to use fast delays (1ms) while production uses sensible defaults (1000ms).

4. **wiremock for HTTP mocking**: wiremock provides excellent HTTP mocking for Rust tests, supporting sequential responses and request matching.

## Critical Learnings for CLAUDE.md

### Persistence Module Patterns (Phase 4)
- **Non-blocking persistence**: Use `try_send` for persistence channel to never block real-time event flow
- **Retry with batch drop**: After configurable max retries (default 3), drop the batch to prevent memory growth
- **Channel-based decoupling**: PersistenceManager runs in background task, receives events via mpsc channel
- **wiremock for HTTP tests**: Use wiremock MockServer for testing HTTP client behavior in Rust

These are general async patterns. The existing CLAUDE.md security patterns remain the most critical project-specific learnings.
