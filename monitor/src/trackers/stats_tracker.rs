//! Stats cache tracker for monitoring Claude Code's token usage statistics.
//!
//! This module watches `~/.claude/stats-cache.json` for changes and emits
//! [`StatsEvent`]s containing both [`SessionMetricsEvent`] and [`TokenUsageEvent`]
//! data.
//!
//! # File Format
//!
//! The stats-cache.json file has the following structure:
//!
//! ```json
//! {
//!   "totalSessions": 150,
//!   "totalMessages": 2500,
//!   "totalToolUsage": 8000,
//!   "longestSession": "00:45:30",
//!   "hourCounts": { "0": 10, "1": 5, ..., "23": 50 },
//!   "modelUsage": {
//!     "claude-sonnet-4-20250514": {
//!       "inputTokens": 1500000,
//!       "outputTokens": 300000,
//!       "cacheReadInputTokens": 800000,
//!       "cacheCreationInputTokens": 100000
//!     }
//!   }
//! }
//! ```
//!
//! # Architecture
//!
//! The tracker uses the [`notify`] crate to watch for file changes, with a 200ms
//! debounce to coalesce rapid file updates. When a change is detected:
//!
//! 1. The JSON file is parsed with retry on failure (file may be mid-write)
//! 2. A [`SessionMetricsEvent`] is emitted once per file read
//! 3. A [`TokenUsageEvent`] is emitted for each model in modelUsage
//!
//! # Example
//!
//! ```no_run
//! use tokio::sync::mpsc;
//! use vibetea_monitor::trackers::stats_tracker::{StatsTracker, StatsEvent};
//!
//! #[tokio::main]
//! async fn main() -> Result<(), Box<dyn std::error::Error>> {
//!     let (tx, mut rx) = mpsc::channel(100);
//!     let tracker = StatsTracker::new(tx)?;
//!
//!     while let Some(event) = rx.recv().await {
//!         match event {
//!             StatsEvent::SessionMetrics(metrics) => {
//!                 println!("Sessions: {}, Messages: {}",
//!                     metrics.total_sessions, metrics.total_messages);
//!             }
//!             StatsEvent::TokenUsage(usage) => {
//!                 println!("Model: {}, Input: {}",
//!                     usage.model, usage.input_tokens);
//!             }
//!         }
//!     }
//!
//!     Ok(())
//! }
//! ```

use std::collections::HashMap;
use std::path::{Path, PathBuf};
use std::sync::Arc;

use notify::{
    event::ModifyKind, Config, Event, EventKind, RecommendedWatcher, RecursiveMode, Watcher,
};
use serde::Deserialize;
use thiserror::Error;
use tokio::sync::mpsc;
use tracing::{debug, error, info, trace, warn};

use crate::types::{SessionMetricsEvent, TokenUsageEvent};
use crate::utils::debounce::Debouncer;

/// Default debounce interval for stats file changes in milliseconds.
const STATS_DEBOUNCE_MS: u64 = 200;

/// Delay before retrying JSON parse on failure in milliseconds.
const PARSE_RETRY_DELAY_MS: u64 = 100;

/// Maximum number of parse retries before giving up.
const MAX_PARSE_RETRIES: u32 = 3;

/// Token usage data for a single model as stored in stats-cache.json.
///
/// Field names match the camelCase JSON format used by Claude Code.
#[derive(Debug, Clone, PartialEq, Eq, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct ModelTokens {
    /// Number of input tokens consumed by this model.
    #[serde(default)]
    pub input_tokens: u64,

    /// Number of output tokens generated by this model.
    #[serde(default)]
    pub output_tokens: u64,

    /// Number of tokens read from the prompt cache.
    #[serde(default)]
    pub cache_read_input_tokens: u64,

    /// Number of tokens written to the prompt cache.
    #[serde(default)]
    pub cache_creation_input_tokens: u64,
}

/// Parsed contents of Claude Code's stats-cache.json file.
///
/// All fields use camelCase to match the JSON format produced by Claude Code.
#[derive(Debug, Clone, PartialEq, Eq, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct StatsCache {
    /// Total number of sessions.
    #[serde(default)]
    pub total_sessions: u64,

    /// Total number of messages across all sessions.
    #[serde(default)]
    pub total_messages: u64,

    /// Total number of tool invocations.
    #[serde(default)]
    pub total_tool_usage: u64,

    /// Duration of the longest session (format: "HH:MM:SS").
    #[serde(default)]
    pub longest_session: String,

    /// Activity counts by hour of day (0-23).
    /// Keys are string representations of hours.
    #[serde(default)]
    pub hour_counts: HashMap<String, u64>,

    /// Token usage broken down by model.
    #[serde(default)]
    pub model_usage: HashMap<String, ModelTokens>,
}

/// Errors that can occur during stats tracking.
#[derive(Error, Debug)]
pub enum StatsTrackerError {
    /// Failed to initialize the file system watcher.
    #[error("failed to create watcher: {0}")]
    WatcherInit(#[from] notify::Error),

    /// Failed to read the stats cache file.
    #[error("failed to read stats cache: {0}")]
    Io(#[from] std::io::Error),

    /// Failed to parse the stats cache JSON.
    #[error("failed to parse stats cache: {0}")]
    Parse(#[from] serde_json::Error),

    /// The Claude directory does not exist.
    #[error("claude directory not found: {0}")]
    ClaudeDirectoryNotFound(PathBuf),

    /// Failed to send event through the channel.
    #[error("failed to send event: channel closed")]
    ChannelClosed,
}

/// Result type for stats tracker operations.
pub type Result<T> = std::result::Result<T, StatsTrackerError>;

/// Events emitted by the stats tracker.
///
/// The stats tracker emits two types of events:
/// - [`TokenUsageEvent`] for each model's token consumption
/// - [`SessionMetricsEvent`] for global session statistics
///
/// Callers can pattern match on this enum to handle each event type.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum StatsEvent {
    /// Token usage event for a specific model.
    TokenUsage(TokenUsageEvent),
    /// Global session metrics event.
    SessionMetrics(SessionMetricsEvent),
}

/// Internal event used to signal a file change to the async processor.
#[derive(Debug, Clone)]
struct FileChangeEvent {
    /// Path to the changed file.
    #[allow(dead_code)]
    path: PathBuf,
}

/// Tracker for Claude Code's stats-cache.json file.
///
/// Watches for file changes and emits [`StatsEvent`]s when the file is
/// updated. Uses debouncing to coalesce rapid file changes.
///
/// # Thread Safety
///
/// The tracker spawns a background task for async processing of file events.
/// Communication is done via channels for thread safety.
#[derive(Debug)]
pub struct StatsTracker {
    /// The underlying file system watcher.
    ///
    /// Kept alive to maintain the watch subscription.
    #[allow(dead_code)]
    watcher: RecommendedWatcher,

    /// Path to the stats-cache.json file.
    stats_path: PathBuf,

    /// Channel sender for emitting stats events.
    #[allow(dead_code)]
    event_sender: mpsc::Sender<StatsEvent>,
}

impl StatsTracker {
    /// Creates a new stats tracker watching the default stats-cache.json location.
    ///
    /// The default location is `~/.claude/stats-cache.json`.
    ///
    /// # Arguments
    ///
    /// * `event_sender` - Channel for emitting [`StatsEvent`]s
    ///
    /// # Errors
    ///
    /// Returns an error if:
    /// - The home directory cannot be determined
    /// - The `~/.claude` directory does not exist
    /// - The file system watcher cannot be initialized
    pub fn new(event_sender: mpsc::Sender<StatsEvent>) -> Result<Self> {
        let claude_dir = directories::BaseDirs::new()
            .map(|dirs| dirs.home_dir().join(".claude"))
            .ok_or_else(|| {
                StatsTrackerError::ClaudeDirectoryNotFound(PathBuf::from("~/.claude"))
            })?;

        Self::with_path(claude_dir.join("stats-cache.json"), event_sender)
    }

    /// Creates a new stats tracker watching a specific stats-cache.json file.
    ///
    /// # Arguments
    ///
    /// * `stats_path` - Path to the stats-cache.json file to watch
    /// * `event_sender` - Channel for emitting [`StatsEvent`]s
    ///
    /// # Errors
    ///
    /// Returns an error if:
    /// - The parent directory of the stats file does not exist
    /// - The file system watcher cannot be initialized
    pub fn with_path(stats_path: PathBuf, event_sender: mpsc::Sender<StatsEvent>) -> Result<Self> {
        // Verify the parent directory exists (file may not exist yet)
        let watch_dir = stats_path
            .parent()
            .ok_or_else(|| StatsTrackerError::ClaudeDirectoryNotFound(stats_path.clone()))?;

        if !watch_dir.exists() {
            return Err(StatsTrackerError::ClaudeDirectoryNotFound(
                watch_dir.to_path_buf(),
            ));
        }

        info!(
            stats_path = %stats_path.display(),
            "Initializing stats tracker"
        );

        // Create debouncer for coalescing file events
        let (debounce_tx, debounce_rx) = mpsc::channel::<(PathBuf, FileChangeEvent)>(100);
        let debouncer: Debouncer<PathBuf, FileChangeEvent> = Debouncer::new(
            std::time::Duration::from_millis(STATS_DEBOUNCE_MS),
            debounce_tx,
        );
        let debouncer = Arc::new(debouncer);

        // Spawn the async processing task
        let sender_for_task = event_sender.clone();
        let stats_path_for_task = stats_path.clone();
        tokio::spawn(async move {
            process_debounced_events(debounce_rx, stats_path_for_task, sender_for_task).await;
        });

        // Create the file watcher
        let watcher = create_watcher(stats_path.clone(), debouncer)?;

        // Do an initial read if the file exists
        if stats_path.exists() {
            let sender_for_init = event_sender.clone();
            let path_for_init = stats_path.clone();
            tokio::spawn(async move {
                if let Err(e) = emit_stats_events(&path_for_init, &sender_for_init).await {
                    warn!(
                        path = %path_for_init.display(),
                        error = %e,
                        "Failed to read initial stats"
                    );
                }
            });
        }

        Ok(Self {
            watcher,
            stats_path,
            event_sender,
        })
    }

    /// Returns the path to the stats-cache.json file being watched.
    #[must_use]
    pub fn stats_path(&self) -> &Path {
        &self.stats_path
    }

    /// Manually triggers a read of the stats file and emits events.
    ///
    /// This is useful for forcing a refresh without waiting for file events.
    ///
    /// # Errors
    ///
    /// Returns an error if the file cannot be read or parsed, or if the
    /// event channel is closed.
    pub async fn refresh(&self) -> Result<()> {
        emit_stats_events(&self.stats_path, &self.event_sender).await
    }
}

/// Creates the file system watcher for the stats cache file.
fn create_watcher(
    stats_path: PathBuf,
    debouncer: Arc<Debouncer<PathBuf, FileChangeEvent>>,
) -> Result<RecommendedWatcher> {
    let watch_dir = stats_path
        .parent()
        .ok_or_else(|| StatsTrackerError::ClaudeDirectoryNotFound(stats_path.clone()))?
        .to_path_buf();

    let stats_filename = stats_path
        .file_name()
        .map(|s| s.to_os_string())
        .unwrap_or_default();

    let mut watcher = RecommendedWatcher::new(
        move |res: std::result::Result<Event, notify::Error>| {
            handle_notify_event(res, &stats_path, &debouncer);
        },
        Config::default(),
    )?;

    // Watch the parent directory since the file may not exist yet
    watcher.watch(&watch_dir, RecursiveMode::NonRecursive)?;

    debug!(
        watch_dir = %watch_dir.display(),
        filename = ?stats_filename,
        "Started watching for stats cache changes"
    );

    Ok(watcher)
}

/// Handles raw notify events and sends them to the debouncer.
fn handle_notify_event(
    res: std::result::Result<Event, notify::Error>,
    stats_path: &Path,
    debouncer: &Debouncer<PathBuf, FileChangeEvent>,
) {
    let event = match res {
        Ok(event) => event,
        Err(e) => {
            error!(error = %e, "File watcher error");
            return;
        }
    };

    trace!(kind = ?event.kind, paths = ?event.paths, "Received notify event");

    // Check if any of the event paths match our stats file
    let matches_stats = event.paths.iter().any(|p| p == stats_path);
    if !matches_stats {
        return;
    }

    // Only process relevant event kinds
    let should_process = matches!(
        event.kind,
        EventKind::Create(_)
            | EventKind::Modify(ModifyKind::Data(_))
            | EventKind::Modify(ModifyKind::Any)
    );

    if !should_process {
        trace!(kind = ?event.kind, "Ignoring event kind");
        return;
    }

    debug!(
        path = %stats_path.display(),
        kind = ?event.kind,
        "Stats cache file changed, sending to debouncer"
    );

    let file_event = FileChangeEvent {
        path: stats_path.to_path_buf(),
    };

    if !debouncer.try_send(stats_path.to_path_buf(), file_event) {
        warn!("Failed to send event to debouncer: channel full or closed");
    }
}

/// Processes debounced file change events.
async fn process_debounced_events(
    mut rx: mpsc::Receiver<(PathBuf, FileChangeEvent)>,
    stats_path: PathBuf,
    sender: mpsc::Sender<StatsEvent>,
) {
    debug!("Starting debounced event processor");

    while let Some((path, _event)) = rx.recv().await {
        if path != stats_path {
            continue;
        }

        debug!(path = %path.display(), "Processing debounced stats change");

        if let Err(e) = emit_stats_events(&path, &sender).await {
            warn!(
                path = %path.display(),
                error = %e,
                "Failed to process stats cache update"
            );
        }
    }

    debug!("Debounced event processor shutting down");
}

/// Reads the stats cache file and emits stats events.
///
/// Emits a [`SessionMetricsEvent`] once per read, followed by a
/// [`TokenUsageEvent`] for each model in the modelUsage section.
///
/// Includes retry logic in case the file is being written to.
async fn emit_stats_events(path: &Path, sender: &mpsc::Sender<StatsEvent>) -> Result<()> {
    let stats = read_stats_with_retry(path).await?;

    // Emit session metrics event first (once per stats-cache.json read)
    let session_metrics = SessionMetricsEvent {
        total_sessions: stats.total_sessions,
        total_messages: stats.total_messages,
        total_tool_usage: stats.total_tool_usage,
        longest_session: stats.longest_session.clone(),
    };

    trace!(
        total_sessions = stats.total_sessions,
        total_messages = stats.total_messages,
        total_tool_usage = stats.total_tool_usage,
        longest_session = %stats.longest_session,
        "Emitting session metrics event"
    );

    sender
        .send(StatsEvent::SessionMetrics(session_metrics))
        .await
        .map_err(|_| StatsTrackerError::ChannelClosed)?;

    // Emit token usage events for each model
    for (model, tokens) in stats.model_usage {
        let event = TokenUsageEvent {
            model: model.clone(),
            input_tokens: tokens.input_tokens,
            output_tokens: tokens.output_tokens,
            cache_read_tokens: tokens.cache_read_input_tokens,
            cache_creation_tokens: tokens.cache_creation_input_tokens,
        };

        trace!(
            model = %model,
            input = tokens.input_tokens,
            output = tokens.output_tokens,
            "Emitting token usage event"
        );

        sender
            .send(StatsEvent::TokenUsage(event))
            .await
            .map_err(|_| StatsTrackerError::ChannelClosed)?;
    }

    Ok(())
}

/// Reads and parses the stats cache file with retry on failure.
///
/// The file may be in the middle of being written, so we retry a few times
/// with a short delay before giving up.
async fn read_stats_with_retry(path: &Path) -> Result<StatsCache> {
    let mut last_error = None;

    for attempt in 0..MAX_PARSE_RETRIES {
        match read_stats(path) {
            Ok(stats) => {
                if attempt > 0 {
                    debug!(
                        path = %path.display(),
                        attempts = attempt + 1,
                        "Successfully parsed stats after retry"
                    );
                }
                return Ok(stats);
            }
            Err(e) => {
                trace!(
                    path = %path.display(),
                    attempt = attempt + 1,
                    error = %e,
                    "Parse attempt failed, will retry"
                );
                last_error = Some(e);
                tokio::time::sleep(std::time::Duration::from_millis(PARSE_RETRY_DELAY_MS)).await;
            }
        }
    }

    Err(last_error
        .unwrap_or_else(|| StatsTrackerError::Io(std::io::Error::other("unknown parse error"))))
}

/// Reads and parses the stats cache file synchronously.
fn read_stats(path: &Path) -> Result<StatsCache> {
    let content = std::fs::read_to_string(path)?;
    let stats: StatsCache = serde_json::from_str(&content)?;
    Ok(stats)
}

/// Parses stats cache JSON from a string.
///
/// This is useful for testing without needing actual files.
pub fn parse_stats_cache(json: &str) -> std::result::Result<StatsCache, serde_json::Error> {
    serde_json::from_str(json)
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::io::Write;
    use tempfile::TempDir;
    use tokio::time::{sleep, timeout, Duration};

    /// Sample stats-cache.json content for testing.
    const SAMPLE_STATS: &str = r#"{
        "totalSessions": 150,
        "totalMessages": 2500,
        "totalToolUsage": 8000,
        "longestSession": "00:45:30",
        "hourCounts": { "9": 50, "14": 100, "17": 75 },
        "modelUsage": {
            "claude-sonnet-4-20250514": {
                "inputTokens": 1500000,
                "outputTokens": 300000,
                "cacheReadInputTokens": 800000,
                "cacheCreationInputTokens": 100000
            },
            "claude-opus-4-20250514": {
                "inputTokens": 500000,
                "outputTokens": 150000,
                "cacheReadInputTokens": 200000,
                "cacheCreationInputTokens": 50000
            }
        }
    }"#;

    /// Creates a temporary directory with a stats-cache.json file.
    fn create_test_stats_file(content: &str) -> (TempDir, PathBuf) {
        let temp_dir = TempDir::new().expect("Failed to create temp dir");
        let stats_path = temp_dir.path().join("stats-cache.json");

        let mut file = std::fs::File::create(&stats_path).expect("Failed to create stats file");
        file.write_all(content.as_bytes())
            .expect("Failed to write stats content");
        file.flush().expect("Failed to flush");

        (temp_dir, stats_path)
    }

    // =========================================================================
    // StatsCache JSON Parsing Tests
    // =========================================================================

    #[test]
    fn test_parse_stats_cache_full() {
        let stats = parse_stats_cache(SAMPLE_STATS).expect("Should parse");

        assert_eq!(stats.total_sessions, 150);
        assert_eq!(stats.total_messages, 2500);
        assert_eq!(stats.total_tool_usage, 8000);
        assert_eq!(stats.longest_session, "00:45:30");
        assert_eq!(stats.hour_counts.len(), 3);
        assert_eq!(stats.hour_counts.get("9"), Some(&50));
        assert_eq!(stats.hour_counts.get("14"), Some(&100));
        assert_eq!(stats.model_usage.len(), 2);
    }

    #[test]
    fn test_parse_model_tokens() {
        let stats = parse_stats_cache(SAMPLE_STATS).expect("Should parse");

        let sonnet = stats
            .model_usage
            .get("claude-sonnet-4-20250514")
            .expect("Should have sonnet");
        assert_eq!(sonnet.input_tokens, 1_500_000);
        assert_eq!(sonnet.output_tokens, 300_000);
        assert_eq!(sonnet.cache_read_input_tokens, 800_000);
        assert_eq!(sonnet.cache_creation_input_tokens, 100_000);

        let opus = stats
            .model_usage
            .get("claude-opus-4-20250514")
            .expect("Should have opus");
        assert_eq!(opus.input_tokens, 500_000);
        assert_eq!(opus.output_tokens, 150_000);
    }

    #[test]
    fn test_parse_empty_stats() {
        let json = "{}";
        let stats = parse_stats_cache(json).expect("Should parse empty object");

        assert_eq!(stats.total_sessions, 0);
        assert_eq!(stats.total_messages, 0);
        assert!(stats.model_usage.is_empty());
        assert!(stats.hour_counts.is_empty());
    }

    #[test]
    fn test_parse_partial_stats() {
        let json = r#"{"totalSessions": 42, "modelUsage": {}}"#;
        let stats = parse_stats_cache(json).expect("Should parse partial");

        assert_eq!(stats.total_sessions, 42);
        assert_eq!(stats.total_messages, 0); // default
        assert!(stats.model_usage.is_empty());
    }

    #[test]
    fn test_parse_model_tokens_missing_fields() {
        let json = r#"{
            "modelUsage": {
                "test-model": {
                    "inputTokens": 100
                }
            }
        }"#;
        let stats = parse_stats_cache(json).expect("Should parse");

        let model = stats
            .model_usage
            .get("test-model")
            .expect("Should have model");
        assert_eq!(model.input_tokens, 100);
        assert_eq!(model.output_tokens, 0); // default
        assert_eq!(model.cache_read_input_tokens, 0); // default
        assert_eq!(model.cache_creation_input_tokens, 0); // default
    }

    #[test]
    fn test_parse_invalid_json() {
        let result = parse_stats_cache("not valid json");
        assert!(result.is_err());
    }

    #[test]
    fn test_parse_malformed_json() {
        let json = r#"{"totalSessions": "not a number"}"#;
        let result = parse_stats_cache(json);
        assert!(result.is_err(), "Should fail on type mismatch");
    }

    // =========================================================================
    // Stats Event Emission Tests
    // =========================================================================

    #[tokio::test]
    async fn test_emit_stats_events_for_each_model() {
        let (_temp_dir, stats_path) = create_test_stats_file(SAMPLE_STATS);

        let (tx, mut rx) = mpsc::channel(100);
        emit_stats_events(&stats_path, &tx)
            .await
            .expect("Should emit events");

        // Should receive 3 events: 1 session metrics + 2 token usage (one for each model)
        let mut received = Vec::new();
        while let Ok(Some(event)) = timeout(Duration::from_millis(100), rx.recv()).await {
            received.push(event);
        }

        assert_eq!(
            received.len(),
            3,
            "Should emit 1 session metrics + 2 token usage events"
        );

        // First event should be session metrics
        let session_metrics = match &received[0] {
            StatsEvent::SessionMetrics(m) => m,
            _ => panic!("First event should be SessionMetrics"),
        };
        assert_eq!(session_metrics.total_sessions, 150);
        assert_eq!(session_metrics.total_messages, 2500);
        assert_eq!(session_metrics.total_tool_usage, 8000);
        assert_eq!(session_metrics.longest_session, "00:45:30");

        // Extract token usage events
        let token_events: Vec<_> = received
            .iter()
            .filter_map(|e| match e {
                StatsEvent::TokenUsage(t) => Some(t),
                _ => None,
            })
            .collect();
        assert_eq!(token_events.len(), 2, "Should have 2 token usage events");

        // Find the sonnet event
        let sonnet = token_events
            .iter()
            .find(|e| e.model == "claude-sonnet-4-20250514")
            .expect("Should have sonnet event");
        assert_eq!(sonnet.input_tokens, 1_500_000);
        assert_eq!(sonnet.output_tokens, 300_000);
        assert_eq!(sonnet.cache_read_tokens, 800_000);
        assert_eq!(sonnet.cache_creation_tokens, 100_000);

        // Find the opus event
        let opus = token_events
            .iter()
            .find(|e| e.model == "claude-opus-4-20250514")
            .expect("Should have opus event");
        assert_eq!(opus.input_tokens, 500_000);
        assert_eq!(opus.output_tokens, 150_000);
    }

    #[tokio::test]
    async fn test_emit_session_metrics_only_for_empty_model_usage() {
        let json = r#"{"totalSessions": 10, "modelUsage": {}}"#;
        let (_temp_dir, stats_path) = create_test_stats_file(json);

        let (tx, mut rx) = mpsc::channel(100);
        emit_stats_events(&stats_path, &tx)
            .await
            .expect("Should succeed");

        // Should receive exactly 1 event (session metrics only)
        let mut received = Vec::new();
        while let Ok(Some(event)) = timeout(Duration::from_millis(100), rx.recv()).await {
            received.push(event);
        }

        assert_eq!(received.len(), 1, "Should emit only session metrics event");

        // Verify it's a session metrics event
        let session_metrics = match &received[0] {
            StatsEvent::SessionMetrics(m) => m,
            _ => panic!("Event should be SessionMetrics"),
        };
        assert_eq!(session_metrics.total_sessions, 10);
    }

    // =========================================================================
    // Debounce Timing Tests
    // =========================================================================

    #[tokio::test]
    async fn test_debounce_coalesces_rapid_events() {
        let (_temp_dir, stats_path) = create_test_stats_file(SAMPLE_STATS);

        let (event_tx, _event_rx) = mpsc::channel(100);
        let (debounce_tx, debounce_rx) = mpsc::channel::<(PathBuf, FileChangeEvent)>(100);

        // Create debouncer with 200ms interval
        let _debouncer: Debouncer<PathBuf, FileChangeEvent> = Debouncer::new(
            std::time::Duration::from_millis(STATS_DEBOUNCE_MS),
            debounce_tx,
        );

        // Spawn processor
        let path_clone = stats_path.clone();
        tokio::spawn(async move {
            process_debounced_events(debounce_rx, path_clone, event_tx).await;
        });

        // Send multiple rapid events to the debouncer
        let (inner_tx, inner_rx) = mpsc::channel::<(PathBuf, FileChangeEvent)>(100);
        let debouncer2: Debouncer<PathBuf, FileChangeEvent> = Debouncer::new(
            std::time::Duration::from_millis(STATS_DEBOUNCE_MS),
            inner_tx,
        );

        // Simulate rapid file events
        for _ in 0..5 {
            debouncer2
                .send(
                    stats_path.clone(),
                    FileChangeEvent {
                        path: stats_path.clone(),
                    },
                )
                .await
                .expect("Should send");
            sleep(Duration::from_millis(10)).await;
        }

        // Drop debouncer to flush
        drop(debouncer2);

        // Collect events from inner debouncer
        let mut inner_events = Vec::new();
        let mut inner_rx = inner_rx;
        while let Ok(Some(event)) = timeout(Duration::from_millis(300), inner_rx.recv()).await {
            inner_events.push(event);
        }

        // Should have coalesced to 1 event
        assert_eq!(
            inner_events.len(),
            1,
            "Rapid events should coalesce to single event"
        );
    }

    #[tokio::test]
    async fn test_debounce_200ms_timing() {
        let (tx, mut rx) = mpsc::channel::<(PathBuf, String)>(100);
        let debouncer: Debouncer<PathBuf, String> =
            Debouncer::new(std::time::Duration::from_millis(200), tx);

        let path = PathBuf::from("/test/stats-cache.json");

        // Send first event
        let start = std::time::Instant::now();
        debouncer
            .send(path.clone(), "event1".to_string())
            .await
            .expect("Should send");

        // Try to receive immediately - should timeout
        let quick_result = timeout(Duration::from_millis(100), rx.recv()).await;
        assert!(
            quick_result.is_err(),
            "Should not receive event before 200ms"
        );

        // Wait for debounce to complete
        let result = timeout(Duration::from_millis(200), rx.recv()).await;
        assert!(result.is_ok(), "Should receive event after debounce");

        let elapsed = start.elapsed();
        assert!(
            elapsed >= Duration::from_millis(200),
            "Should wait at least 200ms, waited {:?}",
            elapsed
        );
    }

    // =========================================================================
    // JSON Parse Retry Tests
    // =========================================================================

    #[tokio::test]
    async fn test_retry_on_parse_failure() {
        let temp_dir = TempDir::new().expect("Failed to create temp dir");
        let stats_path = temp_dir.path().join("stats-cache.json");

        // Create file with invalid JSON initially
        std::fs::write(&stats_path, "invalid json").expect("Should write");

        // Spawn task to fix the file after a delay
        let path_clone = stats_path.clone();
        tokio::spawn(async move {
            sleep(Duration::from_millis(50)).await;
            std::fs::write(&path_clone, SAMPLE_STATS).expect("Should write");
        });

        // read_stats_with_retry should succeed after retry
        let result = read_stats_with_retry(&stats_path).await;
        assert!(result.is_ok(), "Should succeed after retry");
    }

    #[tokio::test]
    async fn test_retry_exhausted() {
        let temp_dir = TempDir::new().expect("Failed to create temp dir");
        let stats_path = temp_dir.path().join("stats-cache.json");

        // Create file with persistently invalid JSON
        std::fs::write(&stats_path, "always invalid").expect("Should write");

        let result = read_stats_with_retry(&stats_path).await;
        assert!(result.is_err(), "Should fail after max retries");
    }

    // =========================================================================
    // Missing/Malformed File Tests
    // =========================================================================

    #[tokio::test]
    async fn test_missing_stats_file() {
        let temp_dir = TempDir::new().expect("Failed to create temp dir");
        let stats_path = temp_dir.path().join("nonexistent.json");

        let (tx, _rx) = mpsc::channel(100);
        let result = emit_stats_events(&stats_path, &tx).await;

        assert!(result.is_err(), "Should fail for missing file");
        assert!(matches!(result.unwrap_err(), StatsTrackerError::Io(_)));
    }

    #[tokio::test]
    async fn test_malformed_json_file() {
        let (_temp_dir, stats_path) = create_test_stats_file("{ broken json");

        let (tx, _rx) = mpsc::channel(100);
        let result = emit_stats_events(&stats_path, &tx).await;

        assert!(result.is_err(), "Should fail for malformed JSON");
    }

    #[tokio::test]
    async fn test_tracker_creation_missing_directory() {
        let (tx, _rx) = mpsc::channel(100);
        let result =
            StatsTracker::with_path(PathBuf::from("/nonexistent/path/stats-cache.json"), tx);

        assert!(result.is_err());
        assert!(matches!(
            result.unwrap_err(),
            StatsTrackerError::ClaudeDirectoryNotFound(_)
        ));
    }

    #[tokio::test]
    async fn test_tracker_creation_with_valid_directory() {
        let temp_dir = TempDir::new().expect("Failed to create temp dir");
        let stats_path = temp_dir.path().join("stats-cache.json");

        let (tx, _rx) = mpsc::channel(100);
        let result = StatsTracker::with_path(stats_path.clone(), tx);

        assert!(result.is_ok(), "Should create tracker for valid directory");
        assert_eq!(result.unwrap().stats_path(), stats_path);
    }

    // =========================================================================
    // StatsTracker Integration Tests
    // =========================================================================

    #[tokio::test]
    async fn test_tracker_initial_read() {
        let (_temp_dir, stats_path) = create_test_stats_file(SAMPLE_STATS);

        let (tx, mut rx) = mpsc::channel(100);
        let _tracker = StatsTracker::with_path(stats_path, tx).expect("Should create tracker");

        // Should receive initial events: 1 session metrics + 2 token usage
        let mut events = Vec::new();
        while let Ok(Some(event)) = timeout(Duration::from_millis(200), rx.recv()).await {
            events.push(event);
        }

        assert_eq!(
            events.len(),
            3,
            "Should receive 1 session metrics + 2 token usage events"
        );

        // Verify we have the right event types
        let session_metrics_count = events
            .iter()
            .filter(|e| matches!(e, StatsEvent::SessionMetrics(_)))
            .count();
        let token_usage_count = events
            .iter()
            .filter(|e| matches!(e, StatsEvent::TokenUsage(_)))
            .count();
        assert_eq!(session_metrics_count, 1, "Should have 1 session metrics");
        assert_eq!(token_usage_count, 2, "Should have 2 token usage events");
    }

    #[tokio::test]
    async fn test_tracker_refresh() {
        let (_temp_dir, stats_path) = create_test_stats_file(SAMPLE_STATS);

        let (tx, mut rx) = mpsc::channel(100);
        let tracker = StatsTracker::with_path(stats_path, tx).expect("Should create tracker");

        // Drain initial events
        while timeout(Duration::from_millis(100), rx.recv()).await.is_ok() {}

        // Call refresh
        tracker.refresh().await.expect("Should refresh");

        // Should receive new events: 1 session metrics + 2 token usage
        let mut events = Vec::new();
        while let Ok(Some(event)) = timeout(Duration::from_millis(100), rx.recv()).await {
            events.push(event);
        }

        assert_eq!(
            events.len(),
            3,
            "Refresh should emit 1 session metrics + 2 token usage events"
        );

        // Verify we have the right event types
        let session_metrics_count = events
            .iter()
            .filter(|e| matches!(e, StatsEvent::SessionMetrics(_)))
            .count();
        let token_usage_count = events
            .iter()
            .filter(|e| matches!(e, StatsEvent::TokenUsage(_)))
            .count();
        assert_eq!(session_metrics_count, 1, "Should have 1 session metrics");
        assert_eq!(token_usage_count, 2, "Should have 2 token usage events");
    }

    // =========================================================================
    // Error Display Tests
    // =========================================================================

    #[test]
    fn test_error_display() {
        let err = StatsTrackerError::ChannelClosed;
        assert_eq!(err.to_string(), "failed to send event: channel closed");

        let err = StatsTrackerError::ClaudeDirectoryNotFound(PathBuf::from("/test"));
        assert_eq!(err.to_string(), "claude directory not found: /test");
    }

    // =========================================================================
    // ModelTokens Equality Tests
    // =========================================================================

    #[test]
    fn test_model_tokens_equality() {
        let a = ModelTokens {
            input_tokens: 100,
            output_tokens: 50,
            cache_read_input_tokens: 25,
            cache_creation_input_tokens: 10,
        };

        let b = ModelTokens {
            input_tokens: 100,
            output_tokens: 50,
            cache_read_input_tokens: 25,
            cache_creation_input_tokens: 10,
        };

        let c = ModelTokens {
            input_tokens: 100,
            output_tokens: 51, // different
            cache_read_input_tokens: 25,
            cache_creation_input_tokens: 10,
        };

        assert_eq!(a, b);
        assert_ne!(a, c);
    }

    // =========================================================================
    // StatsCache Equality Tests
    // =========================================================================

    #[test]
    fn test_stats_cache_clone() {
        let stats = parse_stats_cache(SAMPLE_STATS).expect("Should parse");
        let cloned = stats.clone();

        assert_eq!(stats, cloned);
    }

    // =========================================================================
    // StatsEvent Enum Tests
    // =========================================================================

    #[test]
    fn test_stats_event_enum_variants() {
        let token_event = StatsEvent::TokenUsage(TokenUsageEvent {
            model: "test-model".to_string(),
            input_tokens: 100,
            output_tokens: 50,
            cache_read_tokens: 25,
            cache_creation_tokens: 10,
        });

        let session_event = StatsEvent::SessionMetrics(SessionMetricsEvent {
            total_sessions: 42,
            total_messages: 1000,
            total_tool_usage: 500,
            longest_session: "01:30:00".to_string(),
        });

        // Verify pattern matching works
        match &token_event {
            StatsEvent::TokenUsage(t) => {
                assert_eq!(t.model, "test-model");
                assert_eq!(t.input_tokens, 100);
            }
            _ => panic!("Expected TokenUsage variant"),
        }

        match &session_event {
            StatsEvent::SessionMetrics(m) => {
                assert_eq!(m.total_sessions, 42);
                assert_eq!(m.total_messages, 1000);
            }
            _ => panic!("Expected SessionMetrics variant"),
        }
    }

    #[test]
    fn test_stats_event_equality() {
        let a = StatsEvent::SessionMetrics(SessionMetricsEvent {
            total_sessions: 10,
            total_messages: 100,
            total_tool_usage: 50,
            longest_session: "00:10:00".to_string(),
        });

        let b = StatsEvent::SessionMetrics(SessionMetricsEvent {
            total_sessions: 10,
            total_messages: 100,
            total_tool_usage: 50,
            longest_session: "00:10:00".to_string(),
        });

        let c = StatsEvent::SessionMetrics(SessionMetricsEvent {
            total_sessions: 20, // different
            total_messages: 100,
            total_tool_usage: 50,
            longest_session: "00:10:00".to_string(),
        });

        let d = StatsEvent::TokenUsage(TokenUsageEvent {
            model: "model".to_string(),
            input_tokens: 10,
            output_tokens: 5,
            cache_read_tokens: 2,
            cache_creation_tokens: 1,
        });

        assert_eq!(a, b);
        assert_ne!(a, c);
        assert_ne!(a, d); // Different variants
    }

    #[test]
    fn test_stats_event_clone() {
        let original = StatsEvent::SessionMetrics(SessionMetricsEvent {
            total_sessions: 42,
            total_messages: 1000,
            total_tool_usage: 500,
            longest_session: "01:00:00".to_string(),
        });

        let cloned = original.clone();
        assert_eq!(original, cloned);
    }

    #[test]
    fn test_stats_event_debug() {
        let event = StatsEvent::SessionMetrics(SessionMetricsEvent {
            total_sessions: 42,
            total_messages: 1000,
            total_tool_usage: 500,
            longest_session: "01:00:00".to_string(),
        });

        // Verify Debug trait is implemented and produces output
        let debug_str = format!("{:?}", event);
        assert!(debug_str.contains("SessionMetrics"));
        assert!(debug_str.contains("42"));
    }

    // =========================================================================
    // SessionMetricsEvent Field Mapping Tests
    // =========================================================================

    #[tokio::test]
    async fn test_session_metrics_fields_from_stats_cache() {
        // Test with specific values to ensure proper field mapping
        let json = r#"{
            "totalSessions": 999,
            "totalMessages": 12345,
            "totalToolUsage": 6789,
            "longestSession": "12:34:56",
            "modelUsage": {}
        }"#;
        let (_temp_dir, stats_path) = create_test_stats_file(json);

        let (tx, mut rx) = mpsc::channel(100);
        emit_stats_events(&stats_path, &tx)
            .await
            .expect("Should emit events");

        let event = timeout(Duration::from_millis(100), rx.recv())
            .await
            .expect("Should receive event")
            .expect("Should have event");

        let metrics = match event {
            StatsEvent::SessionMetrics(m) => m,
            _ => panic!("Expected SessionMetrics"),
        };

        assert_eq!(metrics.total_sessions, 999);
        assert_eq!(metrics.total_messages, 12345);
        assert_eq!(metrics.total_tool_usage, 6789);
        assert_eq!(metrics.longest_session, "12:34:56");
    }

    #[tokio::test]
    async fn test_session_metrics_defaults_for_missing_fields() {
        // Empty JSON object should produce default values
        let json = "{}";
        let (_temp_dir, stats_path) = create_test_stats_file(json);

        let (tx, mut rx) = mpsc::channel(100);
        emit_stats_events(&stats_path, &tx)
            .await
            .expect("Should emit events");

        let event = timeout(Duration::from_millis(100), rx.recv())
            .await
            .expect("Should receive event")
            .expect("Should have event");

        let metrics = match event {
            StatsEvent::SessionMetrics(m) => m,
            _ => panic!("Expected SessionMetrics"),
        };

        assert_eq!(metrics.total_sessions, 0);
        assert_eq!(metrics.total_messages, 0);
        assert_eq!(metrics.total_tool_usage, 0);
        assert_eq!(metrics.longest_session, "");
    }

    #[tokio::test]
    async fn test_session_metrics_emitted_before_token_events() {
        let (_temp_dir, stats_path) = create_test_stats_file(SAMPLE_STATS);

        let (tx, mut rx) = mpsc::channel(100);
        emit_stats_events(&stats_path, &tx)
            .await
            .expect("Should emit events");

        // First event should always be SessionMetrics
        let first_event = timeout(Duration::from_millis(100), rx.recv())
            .await
            .expect("Should receive event")
            .expect("Should have event");

        assert!(
            matches!(first_event, StatsEvent::SessionMetrics(_)),
            "First event should be SessionMetrics, got {:?}",
            first_event
        );
    }
}
